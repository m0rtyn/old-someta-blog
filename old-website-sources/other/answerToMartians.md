## Вопрос 1

Я имею ограниченный опыт работы с фреймворками и ещё не осознаю всех болей, которые они призваны решать. Мои боли в написании приложений на данный таковы: - Большая кодовая база требующая структурного усложнения и разбиения на мелкие узкофункциональные куски (модули, слои, звенья). - → Архитектура разделяющая области ответственности на UI, Инструменты, Бизнес-логику. UI-компоненты делятся на умные и глупые, где представление в первых, а функционал -- во вторых. - Динамичность интерфейсов за счёт автомности отдельных частей без обновления всей страницы разом - → Использование шаблонизатора с элементами JavaScript и вируального DOM для просчёта и монтирования изменений в DOM - Потребность в SSR исходящая из применения шаблонизатора для VirtualDOM - → Здесь мои знания сильно ограничены и я склоняюсь к использованию готовых решений - CSS не имеет полноценных областей видимости, что рождает конфликты специфичности на глобальном уровне - → Реализация ShadowDOM для создания кастомных компонентов и инкапсуляции логики со стилями - Асинхронная реактивность - → Сложный пункт, реализацию которого я слабо представляю. Наиболее знакомой мне является концепция реактивного состояния, но сомеваюсь, что остановился бы на ней. В данном случае я бы придерживался функционального подхода для формирования снэпшотов и вычисления разницы между ними для time travel. - Работа в оффлайне и разрешение конфликтов пользовательских изменений затрагивающих данные на сервере - → Этот пункт для меня сверхсложный, я в последнее время много слышу о версионировании и взрослении фронтенда в сторону давно разработанных паттернов (например, git). Но плохо понимаю детали реализации и пути разрешения исключительных конфликтов в случае перебоев связи во время параллельного редактирования данных. - UI-компоненты и их разделение по сложности - → Использование концепции Atomic Design с разбитием на сущности по сложности (атомы, молекулы и тд). А также выше затроннутое разделение на умную и глупую составляющие компонента - Сборка проекта поддерживающая современные импорты, способная делить код на куски и подгружать их по необходимости. Само по себе не проблема, но в сочетании с SSR головная боль. - → Здесь я знаю только то, что есть библиотеки способные решить эту задачу. - Ошибки связанные с типизацией - → Использование строго типизированного языка предотвращающего множество ошибок на этапе компиляции в JavaScript

## Вопрос 2

Эту тему я люблю.
Причины фризов: - Тяжёлые ресурсы страницы, чаще всего это картинки с неверно подобранным форматом (например, PNG24 вместо SVG) - Слишком большое количество запросов к внешним ресурсам (шрифты, стили, изображения, скрипты. - Частый перерасчёт Layout и Paint этапов рендеринга. Это происходит при создании анимаций (в JS или CSS), без учёта производительности свойств. Стоит использовать технику FLIP с использованием свойств transform и opacity. Также полезно свойств will-change, которое позволит браузеру подготовиться к анимации и вынести анимируемый элемент в отдельный слой.

Как поступлю в проекте: - Проверять буду в первую очередь для общей оценки через анализаторы перфоманса и скорости загрузки страницы (PageSpeed, Lighthouse). Затем посмотрю детально через инспектор (вкладки Perfomance, Network). Это мне позволит узнать где именно проседает FPS и соответственно наиболее значимые причины тормозов. - У нас могут происходить излишние вычисления и браузер не будет успевать закончивать их за 10-16мс (для 60 кадров в секунду). При <45 fps анимация не будет восприниматься правной. - Для начала займусь ресурсами страницы, оптимизирую картинки и количество запросов. Добавлю прелоад для критических ресурсов и кеширование.
В CSS и переведу возможные анимации в интерфейсе на FLIP (+свойство will-change). Затем изучу скрипты и узнаю есть ли асинхронная подгрузка данных и можно ли разбить их на куски поменьше. Затем, если использую функцию requestAnimationFrame, которая позволит осуществлять вычисления в пределах 16,6 милисекунд, а также сгруппирует с другими анимациями на экране.
Свойство transform: translateZ(0) -- хак, который позволяет выносить вычисление стилей элемента в отдельный слой и с просчитывать с помощью GPU. Его поведение разниться в зависимости от браузерного движка, но в целом раньше это был хороший способ оптимизации, сейчас достаточно will-change. - Код с unzip я бы оптимизировал за счёт уменьшения частоты рендеринга. Если данных слишком много, то имеет смысл отображать их пачками по 10-100 штук. То есть сначала мы создаём их, а потом монтируем.

    Например:
    ```JS
    var bigJSON = unzip(manyBytesFromLocalStorage)
    var html = ''

    for ( i of bigJSON ) {
      if (i % 100 === 0) {
        html += renderHTML(i)
      }
    }
    ```

## Вопрос 3

Жду решений в области

Жду полноценной и стабильной реализации веб-компонентов, возможности ставить локальную область видимости на стили, SVG 2, CSS color module level 4, Houdini.

## PS

Дописывал ответ на митапе по JS в баре. С начала старался работать итеративно и по помидорам, но всё равно не хватило шести отведённых часов так как изначально первый вопрос поставил меня в ступор. А ещё у меня не было подключения к интернету, поэтому немного запоздал.
